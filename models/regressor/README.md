# Soccer Player Transfer Price Prediction with Ensemble Machine Learning

## Overview

This project uses an ensemble of machine learning models to predict soccer player transfer prices based on player performance statistics and historical transfer data. The project encompasses data preprocessing, feature engineering, model training, hyperparameter optimization using grid search, ensemble modeling with a VotingRegressor, and comprehensive performance evaluation. The ensemble combines optimized Random Forest, XGBoost, and Gradient Boosting regressors.

## Data

### Data Source

The data used for training and evaluation is expected to be located in a CSV file named `transfers_data.csv` within the `data` directory, relative to the notebook's location. This file is generated by running the [`EDA.ipynb`](/EDA.ipynb) notebook, which performs cleaning and preprocessing of the raw data.

**Important:** The specific path used in the notebook to load the CSV file is `"C:\\Users\\devdp\\OneDrive\\Documents\\Github\\UMBC\\soccer_master\\data\\transfers_data.csv"`. You *must* adjust this path within the `cnn.ipynb` notebook to match your local environment for the code to execute correctly.
### Data Preprocessing

The data preprocessing steps, performed in the `regressor.ipynb` notebook, are:

1.  **Loading Data:** Loads the `transfers_data.csv` file using pandas.
2.  **Feature Engineering:**
    *   *Process Formation Data*: Cleans playing formation data by grouping rare formations into 'Other' and handling missing values. Formations with less than 10 occurrences in the training data (seasons before 2021) are grouped as "Other".
    *   *Prepare Model Data*: Splits the data into training and testing sets based on the season year (before 2021 for training, 2021 and later for testing), and identifies categorical and numerical features.
3.  **Feature Transformation:**
    *   *Numerical Features*: Imputes missing values using the median strategy and scales the numerical features using `StandardScaler`.
    *   *Categorical Features*: Imputes missing values with 'missing' and applies one-hot encoding using `OneHotEncoder` with `drop='first'` to avoid multicollinearity.

## Model Training and Optimization

### Individual Regressors

The project optimizes and combines the following regression models:

*   **Random Forest Regressor:** An ensemble learning method that operates by constructing a multitude of decision trees at training time.
    *   *Hyperparameter Tuning:* The number of trees in the forest (`n_estimators`) and the maximum depth of the trees (`max_depth`) are optimized using grid search.
*   **XGBoost Regressor:** An optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.
    *   *Hyperparameter Tuning:* The number of boosting rounds (`n_estimators`), the learning rate (`learning_rate`), and the maximum depth of the trees (`max_depth`) are optimized using grid search.
*   **Gradient Boosting Regressor:** A gradient boosting machine learning algorithm.
    *   *Hyperparameter Tuning:* The number of boosting stages (`n_estimators`), the learning rate (`learning_rate`), and the maximum depth of the trees (`max_depth`) are optimized using grid search.

### Hyperparameter Optimization

Hyperparameter optimization is performed using `GridSearchCV` with the following settings:

*   Cross-validation with 2 folds (`cv=2`).
*   Scoring metric: R-squared (`scoring='r2'`).
*   Parallel processing using all available cores (`n_jobs=-1`).
*   Verbose output during grid search (`verbose=2`).

The specific hyperparameter grids used for each model are:

*   **RandomForestRegressor:**
    ```
    {'n_estimators':[100][300], 'max_depth':[3][5]}
    ```
*   **XGBRegressor:**
    ```
    {'n_estimators':[100][300], 'learning_rate': [0.1, 0.01], 'max_depth':[5]}
    ```
*   **GradientBoostingRegressor:**
    ```
    {'n_estimators':[100][300], 'learning_rate': [0.1], 'max_depth':[5]}
    ```

### Ensemble Model

The optimized individual models are combined using a `VotingRegressor`. The `VotingRegressor` averages the predictions of the individual models to make a final prediction. The ensemble consists of the following models:

*   Random Forest Regressor
*   XGBoost Regressor
*   Gradient Boosting Regressor

## Model Evaluation

### Evaluation Metrics

Model performance is evaluated using the following metrics:

*   **R-squared (R2) Score:** A statistical measure that represents the proportion of the variance for a dependent variable that's explained by a regression model.

### Evaluation Process

1.  The optimized individual models and the ensemble model are evaluated on both the training and test datasets.
2.  R-squared scores are calculated for each model on both datasets.
3.  Results are stored in a pandas DataFrame and printed to the console.
4.  A bar plot is generated to visualize the R-squared scores for each model on the training and test datasets.

### Results

The evaluation results, as shown in the notebook, indicate the following R-squared scores:

** Model Performance Metrics: **

|  | Model | Train_R2 | Test_R2|
|--|-----|--------|--------|
| 0 |Random Forest | 0.589661 | 0.155618
| 1 |XGBoost | 0.707886 | 0.371384
| 2 |Gradient Boosting | 0.686571 | 0.363457
| 3 |Ensemble | 0.700855 | 0.374348


These results suggest that the ensemble model provides the best performance on the test data, achieving an R-squared score of approximately 0.374.

## Usage

1.  **Data Preparation:**
    *   Run the `soccer_master_eda.ipynb` notebook to clean and preprocess the raw data and generate the `transfers_data.csv` file.
    *   Ensure the `transfers_data.csv` file is located in the specified directory, or modify the file path in the `regressor.ipynb` notebook accordingly.
2.  **Run the Notebook:** Execute the `regressor.ipynb` notebook to perform data preprocessing, model training, hyperparameter optimization, ensemble model creation, and evaluation.

## Model Implementation Details

*   The notebook uses `ColumnTransformer` and `Pipeline` from scikit-learn to define and apply preprocessing steps to the data.
*   Hyperparameter optimization is performed using `GridSearchCV` with cross-validation.
*   The ensemble model is created using `VotingRegressor` from scikit-learn.
*   Model persistence is implemented using `joblib` to save the trained models and the preprocessing pipeline to disk.

## Sample Test Case

The notebook includes a function `predict_player_transfer` to predict transfer fees for a specific player. The example in the notebook predicts the transfer fee for Ousmane Dembélé from Dortmund to Barcelona (player ID: 288230).

The predicted and actual transfer fees, along with the absolute difference between them, are printed to the console. The transfer fees are formatted using the `format_currency` function, which adds thousands separators to make the numbers more readable.

The actual transfer was 105,000,000 Euros, while the predicted was 23,939,046 Euros.

## Model Persistence

The trained ensemble model and the fitted feature transformer are saved to disk using `joblib`. The file paths for the saved objects are:

*   Ensemble model: [`ensemble_model.joblib`](ensemble_model.joblib)
*   Feature transformer: [`feature_transformer.joblib`](feature_transformer.joblib)

## Dependencies

*   pandas
*   numpy
*   scikit-learn (sklearn)
*   xgboost
*   matplotlib
*   seaborn
*   joblib

## Additional Notes and Potential Improvements

*   **Feature Engineering:** Further feature engineering could improve model performance. This could include creating interaction terms between existing features or incorporating new data sources.
*   **More Advanced Models:** Experiment with more advanced machine learning models or deep learning techniques.
*   **Hyperparameter Tuning:** Expand the hyperparameter search space to potentially find better model configurations.
*   **Cross-Validation:** Use a more robust cross-validation strategy to obtain more reliable performance estimates.
*   **Regularization:** Explore the impact of regularization techniques to prevent overfitting.
*   **Data Splitting:** Test the model on more recent data to prevent overfitting