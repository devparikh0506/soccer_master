# Soccer Player Transfer Price Prediction with Custom FNN

## Overview

This project implements a custom Feedforward Neural Network (FNN) from scratch using Stochastic Gradient Descent (SGD) optimization to predict soccer player transfer prices based on performance statistics and transfer history. No external neural network libraries were used in the creation of this model. The FNN is built using NumPy.

## Data

### Data Source

The data used for training and evaluation is expected to be located in a CSV file named `transfers_data.csv` within the `data` directory, relative to the notebook's location. This file is generated by running the [`EDA.ipynb`](/EDA.ipynb) notebook, which performs cleaning and preprocessing of the raw data.

**Important:** The specific path used in the notebook to load the CSV file is `"C:\\Users\\devdp\\OneDrive\\Documents\\Github\\UMBC\\soccer_master\\data\\transfers_data.csv"`. You *must* adjust this path within the `cnn.ipynb` notebook to match your local environment for the code to execute correctly.

### Data Preprocessing

The data preprocessing steps, performed in the `cnn.ipynb` notebook, are:

1.  **Loading Data:** Loads the `transfers_data.csv` file using pandas.
2.  **Feature Engineering:**
    *   *Process Formation Data*: Cleans playing formation data by grouping rare formations into 'Other' and handling missing values. Formations with less than 10 occurrences in the training data (seasons before 2021) are grouped as "Other".
    *   *Prepare Model Data*: Splits the data into training and testing sets based on the season year (before 2021 for training, 2021 and later for testing), and identifies categorical and numerical features.
3.  **Feature Transformation:**
    *   *Numerical Features*: Imputes missing values using the median strategy and scales the numerical features using `StandardScaler`.
    *   *Categorical Features*: Imputes missing values with 'missing' and applies one-hot encoding using `OneHotEncoder` with `drop='first'` to avoid multicollinearity.
4.  **Target Scaling:** Scales the target variable (`transfer_fee`) using `StandardScaler`.

## Model Architecture

The model is a custom-built Feedforward Neural Network (FNN) without the use of neural network libraries.

*   The model architecture includes the following layers:
    *   Linear Layer (input dimension 66, output dimension 40)
    *   ReLU activation
    *   Linear Layer (input dimension 40, output dimension 20)
    *   ReLU activation
    *   Linear Layer (input dimension 20, output dimension 1) which outputs the predicted transfer fee
    *   LinearWithMSE layer with Mean Squared Error Loss to compare prediction with true values

## Training

The model is trained using Stochastic Gradient Descent (SGD). The training process involves feeding the pre-processed data in batches, calculating the loss using Mean Squared Error, and updating the weights and biases of the network to minimize the loss.

**Training Parameters:**

*   **Optimizer:** SGD
*   **Learning Rate:** 0.1
*   **Epochs:** 20
*   **Batch Size:** 256

The training process shuffles data each epoch and uses a custom-written `data_generator` to create batches. Training loss is printed after each epoch.

## Dependencies

*   pandas
*   scikit-learn (sklearn)
*   numpy

## Usage

1.  **Data Preparation:**
    *   Run the `soccer_master_eda.ipynb` notebook to clean and preprocess the raw data and generate the `transfers_data.csv` file.
    *   Ensure the `transfers_data.csv` file is located in the specified directory, or modify the file path in the `cnn.ipynb` notebook accordingly.
2.  **Run the Notebook:** Execute the `cnn.ipynb` notebook to perform data preprocessing, model training, and evaluation.

## Model Implementation Details

*   **Layers:**
    *   `Linear`: Implements a fully connected linear layer with weights (W) and biases (b). Weights are initialized using normalized random values.
    *   `ReLU`: Implements the Rectified Linear Unit activation function.
    *   `LinearWithMSE`: Implements a linear output layer with Mean Squared Error (MSE) loss.
*   **Optimizer:**
    *   `SGD`: Implements the Stochastic Gradient Descent optimization algorithm. The learning rate is set to 0.1.
*   **Training Loop:**
    *   The training loop iterates over the training data for a specified number of epochs.
    *   Within each epoch, the data is shuffled, and the model processes the data in batches using the `data_generator`.
    *   For each batch, the model performs a forward pass to calculate the loss, a backward pass to compute gradients, and updates the model parameters using the SGD optimizer.
*   **Data Generator:**
    *   The `data_generator` function efficiently yields batches of data and labels for training and testing. It handles cases where the number of samples is not evenly divisible by the batch size.
*   **Prediction:**
    *   The `predict_player_transfer` function demonstrates how to use the trained model to predict transfer fees for specific players. It takes a player ID as input, retrieves the player's data, preprocesses the features, and uses the trained model to predict the transfer fee.

## Performance

The model's performance is evaluated based on a held-out test set (years 2021 and after). The training loss decreases significantly over epochs.

**Test Results:**
*   Mean Absolute Error (MAE): 33600.3867
*   Root Mean Squared Error (RMSE): 86187.7947
*   R2 Score: 0.9998

## Additional Notes

*   The notebook includes a function `predict_player_transfer` to predict transfer fees for individual players. This function also prints the actual transfer fee and the difference between the predicted and actual fees.  It requires the `player_id`, the data DataFrame, the trained `model`, the fitted `transformer`, and the lists of categorical and numerical columns.
*   The `format_currency` function provides a convenient way to display transfer fees in a readable format with thousands separators.